\documentclass[12pt]{report}

\usepackage[utf8]{inputenc}
\usepackage{stmaryrd}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{hyperref}

\title{Résolution de l'équation de précession d'un moment magnétique par réseaux de neurones}

\author{
  Matthieu Carreau\\
  Telecom Paris, Institut Polytechnique de Paris\\
  F-91120, Palaiseau, France\\
  \texttt{matthieu.carreau@telecom-paris.fr} \\
  [1em] Supervisors: \\
  Stam Nicolis\\
  Institut Denis Poisson\\
  Université de Tours, Université d'Orléans, CNRS (UMR7013)\\
  Parc de Grandmont, F-37200, Tours, France\\
  \texttt{stam.nicolis@lmpt.univ-tours.fr}\\
  [1em] \\
  Pascal Thibaudeau\\
  CEA Le Ripault\\
  BP 16, F-37260, Monts, France\\
  \texttt{pascal.thibaudeau@cea.fr}
}
\date{Juillet 2022}

\begin{document}

\maketitle

\begin{abstract}
    Ce rapport présente une méthode numérique permettant l'étude de la préces\-sion d'un moment magnétique dans un champ extérieur en présence d'un amortissement.
    On décrira et mettra en {\oe}uvre des méthodes d'optimisation pour déterminer des solutions approchées d'équations différentielles, éventuellement non-linéaires, en fixant un modèle de solutions impliquant un réseau de neurones. 
    En particulier, on montrera une méthode de descente de gradients qui permet de minimiser une fonction de coût servant à approcher la solution de l'équation différentielle.
    La mise en {\oe}uvre numérique sera ensuite comparée à une suite de problèmes formée par ordre de difficulté croissante. 
    D'une part nous chercherons à retrouver une primitive d'une fonction trigonométrique, puis nous évaluerons la solution d'un système de deux équations différentielles couplées, décrivant la précession de Larmor.
    Enfin nous évaluerons notre approche sur l'équation de Landau-Lifshitz-Gilbert, qui décrit le phénomène d'amortissement.
\end{abstract}
    
\tableofcontents{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chap:introduction}

Ce rapport est le compte-rendu d'un stage d'un mois à \href{https://www.idpoisson.fr}{l'Institut Denis Poisson de Tours} ayant pour objet la mise en place et l'implémentation de méthodes de résolution d'équations différentielles à l'aide de réseaux de neurones. 
Le problème physique étudié est celui de la précession d'un moment magnétique dans un champ magnétique, que l'on décrira dans un premier temps par l'équation de précession de Larmor~\cite{PrecessionLarmor}, puis par l'équation de Landau-Lifshitz-Gilbert~\cite{EquationGilbert}.
Cette seconde version de l'équation possède une solution analytique obtenue par projection transverse dans le cas d'une precession dans un champ statique et homogène mais on ne lui connaît pas de solution analytique en dehors de ce cas.
C'est pourquoi il est pertinent de se pencher sur des méthodes numériques pour en obtenir des solutions approchées.

L'approche proposée ici consiste à utiliser des réseaux de neurones, connus pour leur capacité à approcher des fonctions~\cite{FunctionApproximation}, pour trouver des fonctions "proches" d'être solution d'une équation différentielle.
On utilise par conséquent les réseaux de neurones en suivant les références~\cite{MLWithApp,ANNforOPDEs,HighOrderHybrid}, en utilisant les algorithmes d'optimisation connus pour minimiser une fonction de coût (désignée aussi sous la nom de fonction erreur), qui indique dans quelle mesure une fonction candidate, construite à partir du réseau de neurones, est proche de la solution.

On écrira lorsque cela sera possible l'équation différentielle d'inconnue $\bm{M}: \mathbb{R} \to  \mathbb{R}^n$ (vecteur adimensionné représentant le moment magnétique) sous une forme similaire à la suivante, où $\bm{g}:  \mathbb{R}\times\mathbb{R}^n \to  \mathbb{R}^n$ est une fonction connue :

\begin{equation}
    \left\{
        \begin{aligned}
            \frac{d\bm{M}(t)}{dt} &= \bm{g}(t, \bm{M}(t)) \\
            \bm{M}(0) &= \bm{M}_0
        \end{aligned}
    \right.
\label{eq:equa_dif_example}
\end{equation}

La méthode consiste ensuite à construire une fonction candidate pour l'équation différentielle comme une somme de deux termes.
Le premier est un terme non ajustable vérifiant la condition initiale, et le deuxième dépendant des paramètres à entraîner représentés par le vecteur $\bm{P}$. Ce dernier est construit de façon à ne pas influencer la valeur initiale de la fonction, afin que la condition initiale soit toujours satisfaite, ceci étant réalisé grâce à une fonction $f:\mathbb{R}\to\mathbb{R}$ fixée à l'avance (souvent $f = id_{\mathbb{R}}$)

\begin{equation}
    \bm{\Tilde{M}}(t) = \bm{M}_0 + f(t)\mathcal{N}(t,\bm{P}) 
\label{eq:solution_candidate_generique}
\end{equation}
    
On définit ensuite une fonction de coût, que l'on calcule à partir de $N$ points $(t_i)_{i\in \llbracket 1, N \rrbracket}$ de l'intervalle d'étude, de la forme :

\begin{equation}
    E ({\bm P})= \sum_{i=1}^{N} (\frac{d\bm{\Tilde{M}}}{dt}(t_i) -g(t_i, \bm{\Tilde{M}}(t_i)))^2   
\label{eq:fonction_erreur_generique}
\end{equation}

Il s'agira enfin d'utiliser différentes méthodes d'optimisation propres aux réseaux de neurones afin de trouver un vecteur $\bm P$ qui minimise (et idéalement annule) cette fonction d'erreur.

Dans le chapitre~\ref{chap:ode_1}, on s'intéressera à résoudre une équation différentielle à une dimension, en choisissant d'abord le terme ajustable $\mathcal{N}(t,\bm{P})$ sous la forme d'une série de Fourier, puis comme la sortie d'un réseau de neurones.
On appliquera dans le chapitre~\ref{chap:precession} les mêmes méthodes pour la résolution d'un système de deux équations différentielles couplées représentant le mouvement de précession décrit par l'équation de Larmor.
On étudiera ensuite dans le chapitre~\ref{chap:gilbert} l'effet de l'ajout du terme d'amortissement introduit dans l'équation de Landau-Lifshitz-Gilbert.
Enfin dans le chapitre~\ref{chap:conclusion}, je discuterai des résultats obtenus et proposerai quelques perspectives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Equation différentielle à une dimension}
\label{chap:ode_1}

On souhaite se familiariser avec différentes méthodes numériques appliquées à la résolution d'une équation différentielle d'ordre 1 en une dimension.

On considère alors $M$, une fonction réelle à une variable satisfaisant l'équation différentielle suivante, où $M_0$ désigne la valeur initiale de la fonction :

\begin{equation}
    \left\{
    \begin{aligned}
        \frac{dM(t)}{dt} + \cos(2\pi t) & = 0\\
        M(0)                            & = M_0
    \end{aligned}
    \right.
    \label{eq:equa_dif}
\end{equation}

Cherchons à mettre en {\oe}uvre les méthodes présentées dans la section~\ref{chap:introduction} sur l'équation~(\ref{eq:equa_dif}), pour tout $t\in [t_a=0,t_b=1]$.
L'équation~(\ref{eq:equa_dif}) et sa condition initiale donnée en $t=0$, admettent une solution analytique unique qui s'écrit
\begin{equation}
    {M}(t) = M_0 - \frac{1}{2\pi}\sin(2\pi t)
    \label{eq:solution_analytique}
\end{equation}

L'équation~(\ref{eq:solution_analytique}) permettra d'évaluer la qualité des solutions numériques trouvées par différence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution en séries de Fourier}
\label{sec:sol_ser_fourier_1d}

On cherche des solutions numériques approchées de l'équation~(\ref{eq:equa_dif}) sous la forme de séries de Fourier tronquées en un nombre entier $H$ d'harmoniques.
On écrit pour cela la solution approchée $\Tilde{M}$ comme la somme de deux termes: 
\begin{itemize}
    \item le premier terme est la constante $M_0$, paramètre non ajustable vérifiant la condition initiale,
    \item le deuxième terme est $\mathcal{N}(t,\bm{A})$, une fonction temporelle développée sur une base et dépendant d'un vecteur ${\bm A}$ de dimension $H$, dont les valeurs sont notées $(A_m)_{m\in \llbracket 1,H \rrbracket}$.
    Cette fonction est construite de façon à ne pas influencer la condition initiale de la fonction.
\end{itemize}
Ainsi on s'assure en particulier que $\mathcal{N}(0,\bm{A})=0$ de sorte que la solution approchée coïncide avec la solution exacte à $t=0$. 
En équations cela donne
\begin{equation}
    \left\{
    \begin{aligned}
        \Tilde{M}(t)            & = M_0 + {\mathcal{N}}(t,\bm{A})     \\
        {\mathcal{N}}(t,\bm{A}) & = \sum_{m=1}^{H} A_m \sin(2\pi m t)
    \end{aligned}
    \right.
    \label{eq:solution_fourier}
\end{equation}

On définit une fonction de coût $E$ à minimiser dans un espace $L^2$, obtenue à partir de la valeur de la dérivée de $\Tilde{M}$ sur un intervalle temporel de $N$ points pour $t$. 
Nous posons $\forall i \in\llbracket 1,N \rrbracket, t_i = \frac{i-1}{N} $ :

\begin{equation}
    \begin{aligned}
        E & = \frac{1}{2}\sum_{i=1}^{N} (\frac{d\Tilde{M}}{dt}(t_i) +\cos(2\pi t_i))^2               \\
          & = \frac{1}{2}\sum_{i=1}^{N}(\sum_{m=1}^{H} 2\pi m A_m \cos(2\pi m t_i)+\cos(2\pi t_i))^2
    \end{aligned}
    \label{eq:fonction_erreur}
\end{equation}

Puisque le vecteur $\bm{A}$ représente maintenant un ensemble de paramètres ajustables, on cherche le minimum de $E$ en tant que fonction de $\bm{A}$.
Une condition nécessaire sur $\bm{A}$ pour être un antécédent d'un minimum est
\begin{equation}
    \frac{\partial E}{\partial A_l} = 0, {\forall l \in\llbracket 1,H \rrbracket}
    \label{eq:condition_necessaire_A_l}
\end{equation}

On doit maintenant déterminer les dérivées partielles de $E$ par rapport à chacun des coefficients de $\bm{A}$.
Ces dérivées partielles sont données pour $l \in \llbracket 1,H \rrbracket$ par :

\begin{equation}
    \frac{\partial E}{\partial A_l} =
    \sum_{i=1}^{N}(\sum_{m=1}^{H} 2\pi m A_m \cos(2\pi m t_i)+\cos(2\pi t_i))
    2\pi l \cos(2\pi l t_i)
    \label{eq:gradient}
\end{equation}

Nous devons donc trouver les coefficients $A_m$ avec ${m\in \llbracket 1,H \rrbracket}$ qui vérifient la condition \ref{eq:condition_necessaire_A_l}, et vérifier que le vecteur des coefficients trouvés correspond bien à la solution analytique, i.e $\forall m \in\llbracket 1,H \rrbracket, A_m = -\frac{1}{2\pi}\delta_1^m$.
Pour cela nous allons développer deux approches.
Dans la section~\ref{sec:inversion_sys_lin} nous inverserons un système linéaire et dans la section~\ref{sec:algo_grad} nous formerons un algorithme fondé sur une descente de gradients.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inversion d'un système linéaire}
\label{sec:inversion_sys_lin}

Pour la fonction approchée $\tilde{M}$, la condition \ref{eq:condition_necessaire_A_l} qui est équivalente à l'expression~\ref{eq:gradient}, est en réalité un système linéaire de $H$ équations, que l'on peut représenter en définissant les matrices suivantes :

\begin{equation}
    \mathcal{M} = (r_{m,l})_{(m,l)\in \llbracket 1, H\rrbracket ^2},
    \bm{A} = \begin{pmatrix}
        A_1    \\
        A_2    \\
        \vdots \\
        A_H
    \end{pmatrix},
    \bm{b} = \begin{pmatrix}
        b_1    \\
        b_2    \\
        \vdots \\
        b_H
    \end{pmatrix}
    \label{eq:definition_notation}
\end{equation}

\begin{equation}
    \forall (m,l) \in \llbracket 1, N\rrbracket ^2,
    \left\{
    \begin{aligned}
        r_{m,l} & = 2\pi ml \sum_{i=1}^{N}\cos(2\pi mt_i)\cos(2\pi lt_i) \\
        b_l     & = -l\sum_{i=1}^{N}\cos(2\pi t_i)\cos(2\pi lt_i)
    \end{aligned}
    \right.
    \label{eq:definition_coefficients}
\end{equation}

On résoud ce système linéaire en écrivant l'équation matricielle qui le représente, c'est-à-dire :

\begin{equation}
    \mathcal{M} \bm{A} = \bm{b} \Leftrightarrow \bm{A} = \mathcal{M}^{-1}\bm{b}
    \label{eq:equation_matricielle}
\end{equation}

Il ne reste plus maintenant qu'à réaliser l'implémentation numérique associée en python, en instanciant la matrice ${\mathcal{M}}$ et les vecteurs $\bm{A}$ et $\bm{b}$ à l'aide de la bibliothèque \href{https://numpy.org}{\emph{numpy}}.

Malheureusement la matrice $\mathcal{M}$ n'est pas toujours inversible selon le choix de $H$ et $N$.
En effet, on peut montrer que c'est nécessairement le cas lorsque $H>N$. 
Il suffit pour cela d'écrire la matrice $\mathcal{M}$ en fonction de la matrice $\mathcal{A}$ de taille $N \times H$, dont les coefficients sont données par $ \forall (i,j) \in \llbracket 1, N\rrbracket \times \llbracket 1, H\rrbracket, a_{i,j} = j \cos(2 \pi j t_i)$.
On a alors $\mathcal{M} = 2\pi(\mathcal{A}^T)\mathcal{A}$. 
On déduit un majorant du rang de $\mathcal{M}$: $\mathrm{rg}(\mathcal{M}) \leq \min(\mathrm{rg}(\mathcal{A}^T), \mathrm{rg}(\mathcal{A})) = \mathrm{rg}(\mathcal{A}) \leq N $. 
Ainsi si $N<H$, $\mathcal{M}$ n'est pas inversible.

De plus, cette matrice n'est pas non plus inversible lorsque $H$ et $N$ sont proches l'un de l'autre, par exemple pour $H=N=10$.
Il serait intéressant d'approfondir cet aspect pour savoir si cela se traduit par le fait que la fonction $E$ admette plusieurs minima locaux.

En réalisant des expériences numériques, il semble que choisir $N\gg H$ soit suffisant pour que $\mathcal{M}$ soit inversible.
On choisira $H=10, N=100$ pour la suite.
On obtient alors les coefficients présentés dans la figure \ref{fig:resultats_1_inv}.b avec les valeurs absolues des erreurs de chacun des coefficients trouvés par rapport à leur valeur exacte (Figure \ref{fig:resultats_1_inv}.b).
On constate que les erreurs sur chaque coefficient est inférieure à $10^{-16}$, ce qui représente la précision machine.
On valide donc cette première méthode.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth,height=0.9\textwidth]{coefs_1_inv.jpg}
        \caption{Valeur des coefficients $A_l$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth,height=0.9\textwidth]{coefs_1_inv_erreur.jpg}
        \caption{Erreur des coefficients $A_l$}
    \end{subfigure}
    \caption{Détermination des $10$ coefficients du vecteur $\bm{A}$ sur une base temporelle donnée, par inversion d'un système linéaire.}
    \label{fig:resultats_1_inv}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithme de descente de gradients}
\label{sec:algo_grad}

Profitons de cet exemple simple où le système est linéaire pour se familiariser avec l'algorithme de descente de gradients qui sera utile par la suite.
Le principe est de  partir d'un vecteur $\bm{A}^{(0)}$ choisit aléatoirement puis de calculer itérativement un nouveau vecteur $\bm{A}^{(k)}$ en se déplaçant dans l'espace dans la direction qui minimise le plus l'erreur localement.
On définit pour cela les paramètres suivants :

\begin{equation}
    \alpha>0,
    \bm{A}^{(0)} = \begin{pmatrix}
        A_1^{(0)} \\
        A_2^{(0)} \\
        \vdots    \\
        A_H^{(0)}
    \end{pmatrix},
    \bm{g}^{(0)} = \begin{pmatrix}
        \frac{\partial E^{(0)}}{\partial A_1^{(0)}} \\
        \frac{\partial E^{(0)}}{\partial A_2^{(0)}} \\
        \vdots                                      \\
        \frac{\partial E^{(0)}}{\partial A_H^{(0)}}
    \end{pmatrix},
    \label{eq:definition_descente_gradients}
\end{equation}

Puis on calcule itérativement :

\begin{equation}
    \bm{A}^{(k+1)} = \bm{A}^{(k)} - \alpha\bm{g}^{(k)}
    \label{eq:recurrence_descente_gradients}
\end{equation}

On cherche à trouver le coefficient $\alpha$ optimal qui assure la convergence tout en maximisant la vitesse de convergence.
On exprime tout d'abord le gradient en fonction de la matrice $\mathcal{M}$ et du vecteur $\bm{b}$ définis précédemment qui sont indépendants de $\bm{A}$ et de $k$:

\begin{equation}
    \bm{g}^{(k)} = \mathcal{M}\bm{A}^{(k)} - \bm{b}
    \label{eq:expression_matricielle_gradient}
\end{equation}

Ainsi, l'équation de récurrence (\ref{eq:recurrence_descente_gradients}) se réécrit comme une suite arithmético-géométrique de vecteurs :

\begin{equation}
    \bm{A}^{(k+1)} = (\mathcal{I}_H - \alpha \mathcal{M} )  \bm{A}^{(k)} + \alpha\bm{b}
    \label{eq:recurrence_descente_gradients_v2}
\end{equation}

Cette équation de récurrence peut se réécrire si $\mathcal M$ est inversible en posant $\bm{A^*} = \mathcal {M}^{-1}\bm{b}$ (qui est l'unique solution du système linéaire) on arrive à l'expression de $\bm{A}^{(k)}-\bm{A^*} $ comme une suite géométrique :

\begin{equation}
    \begin{aligned}
        \bm{A}^{(k+1)}-\bm{A^*} & = (\mathcal{I}_H - \alpha \mathcal{M} )  \bm{A}^{(k)} + \alpha\bm{b} - \mathcal {M}^{-1}\bm{b} \\
                                & = (\mathcal{I}_H - \alpha \mathcal{M} ) (\bm{A}^{(k)}-\bm{A^*} ) \\
                                & = (\mathcal{I}_H - \alpha \mathcal{M} )^k (\bm{A}^{(0)}-\bm{A^*} )
    \end{aligned}
    \label{eq:recurrence_descente_gradients_2D_v2}
\end{equation}

Pour s'assurer de la convergence de cette suite, on s'intéresse au rayon spectral, c'est-à-dire le maximum du module des valeurs propres, de $\mathcal{R}_\alpha = (\mathcal{I}_H - \alpha \mathcal{M} )$, noté $\rho(\mathcal{R}_\alpha)$.
La suite $(\bm{A}^{(k)})_{k\in \mathbb{N}}$ converge si et seulement si la norme de $(\mathcal{R}_\alpha ^k)_{k\in \mathbb{N}}$ tend vers 0, sa limite est alors $\bm A^*$.
Cela est le cas si et seulement si $\rho(\mathcal{R}_\alpha)<1$ ~\cite{WikiRayonSpectral}.
De plus, elle convergera d'autant plus vite que son rayon spectral est faible.
On trace donc ce maximum en fonction de $\alpha$ en figure \ref{fig:choix_alpha_1D}.
On en déduit la valeur critique $\alpha_c = 6.2807.10^{-5}$, pour laquelle $\rho(\mathcal{R}_\alpha)=1$, ainsi que la valeur $\alpha_{min} = 6.2189.10^{-5}$ pour $\rho(\mathcal{R}_\alpha)$ est minimal.

On éxécute l'algorithme en parallèle pour les deux valeurs de $\alpha$ trouvées précédemment ainsi que pour une valeur $\alpha_1 = 6.3.10^{-5}$, tel que $\rho(\mathcal{R}_\alpha)>1$
On montre l'évolution de l'erreur en fonction du nombre d'itérations en figure \ref{fig:erreur_selon_alpha}.
On constate que $\alpha_{min}$ donne lieu à une décroissance exponentielle de l'erreur pendant les 2000 premières itérations, qui devient ensuite stationnaire.
$\alpha_1$ donne une erreur qui croît exponentiellement car la norme de $(\mathcal{R}_\alpha ^n)_{n\in \mathbb{N}}$ diverge exponentiellement.
La valeur $\alpha_c$ donne une erreur constante, et correspond au cas limite entre les 2 cas précédents.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth, height=0.8\textwidth]{choix_alpha_1D.jpg}
        \caption{$\alpha \in [0, 10^{-4}]$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth, height=0.8\textwidth]{choix_alpha_1D_zoom.jpg}
        \caption{Au voisinage de l'intersection avec 1}
    \end{subfigure}
    \caption{Rayon spectral de $\mathcal{R}_\alpha$ en fonction de $\alpha$}
    \label{fig:choix_alpha_1D}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{comparaison_erreurs_selon_alpha_1D.jpg}
    \caption{Erreurs en fonction du nombre d'itérations pour 3 valeurs de $\alpha$}
    \label{fig:erreur_selon_alpha}
\end{figure}


On retient donc les résultats obtenus pour $\alpha_{min}$ que l'on montre en figure \ref{fig:resultats1DG} avec les valeurs absolues des erreurs de chacun par rapport à la valeur théorique.
On constate que les erreurs sur chaque coefficient est inférieure à $10^{-16}$, soit la précision machine et on valide donc cette seconde méthode.

Cette étude a mis en évidence l'importance du choix du taux d'apprentissage qui détermine le comportement de la suite de vecteurs obtenus lors de la descente de gradients.
Cela sera important à garder en tête par la suite, même quand les systèmes à résoudre ne seront plus linéaire et qu'il ne sera plus possible de déterminer à l'avance les valeurs optimales.
Elles devront donc être testées empiriquement.


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{coefs_1_DG.jpg}
        \caption{Coefficients trouvés}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{coefs_1_DG_erreur.jpg}
        \caption{Valeurs absolue de l'erreur pour chaque coefficient}
    \end{subfigure}
    \caption{Résultats de la méthode de descnte de gradients}
    \label{fig:resultats1DG}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solutions par réseau de neurones}

On cherche à présent à utiliser un réseau de neurones pour approcher la solution de l'équation différentielle. On cherche désormais des solutions approchées sous la forme suivante :

\begin{equation}
    \left\{
    \begin{aligned}
        \Tilde{M}(t)            & = M_0 + t{\mathcal{N}}(t,\bm{P})      \\
        {\mathcal{N}}(t,\bm{P}) & = \sum_{j=1}^{H} v_j \sigma(w_jt+b_j)
    \end{aligned}
    \right.
    \label{eq:solution_NN}
\end{equation}

Le terme ${\mathcal{N}}(t,\bm{P})$ correspond donc à la sortie d'un réseau de neurones dont l'architecture est présentée en figure \ref{fig:NN}, contenant une couche cachée intermédiaire, qui réalise en sortie une somme pondérée de sigmoïdes, la fonction utilisée est ${\displaystyle{\forall x \in \mathbf{R}, \sigma(x) = \frac{1}{1+e^{-x}}}}$.
Les paramètres $\bm{P}$ à ajuster sont désormais les coefficients $(w_j)_{j\in \llbracket 1,H \rrbracket}$, $(b_j)_{j\in \llbracket 1,H \rrbracket}$ et $(v_j)_{j\in \llbracket 1,H \rrbracket}$.

Pour plus de détails, la page~\cite{WikiNN} présente le principe de ce type de réseau de neurones.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{NN.jpg}
    \caption{Réseau de neurones}
    \label{fig:NN}
\end{figure}

On définit une nouvelle fonction d'erreur, calculée à partir des $N$ points suivants : $\forall i \in\llbracket 1,N \rrbracket, t_i = \frac{i-1}{N-1} $

\begin{equation}
    E(\bm{P}) = \sum_{i=1}^{N} (\frac{d\Tilde{M}}{dt}(t_i) + \cos(2\pi t_i))^2
    \label{eq:erreur_NN}
\end{equation}


On calcule ensuite les expressions analytiques des dérivées partielles de $E(\bm{P})$ par rapport à chaque paramètre ajustable, puis on cherche à minimiser cette erreur à l'aide de l'algorithme de descente de gradients.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Résultats obtenus}

On initialise l'algorithme avec les paramètres suivants : $(H=4, N=20)$
On obtient une erreur de $1,2.10^{-2}$ et une estimation visible en figure \ref{fig:resultat_NN}.
Cela permet de valider notre modèle sur l'étude à une dimension.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{resultat_NN.png}
    \caption{estimation de la solution par un réseau de neurones}
    \label{fig:resultat_NN}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Mouvement de précession}
\label{chap:precession}

On s'intéresse désormais à l'équation de la précession de Larmor~\cite{PrecessionLarmor} pour un moment magnétique $\bm{M} \in \mathbb{R}^3$ autour d'un vecteur ${\bm{\omega}}$ pointant le long de l'axe $z$ et dont la composante sur cet axe est constante, soit $\bm{\omega}=\omega_z{\bm z}$.

\begin{equation}
    \frac{d\bm{M}}{dt} = \bm{\omega}\times\bm{M}
    \label{eq:equation_Larmor}
\end{equation}

Cette équation vectorielle implique que $\frac{d\bm{M}}{dt} $ et $\bm{\omega}$ sont deux vecteurs orthogonaux, ainsi $\frac{dM_z}{dt}=0$.
Par conséquent, on ne considérera que les composantes selon les vecteurs $\bm{x}$ et $\bm{y}$ et l'équation~\ref{eq:equation_Larmor} se réécrit alors comme un système de deux équations différentielles couplées :

\begin{equation}
    \left\{
    \begin{aligned}
        \frac{dM_x}{dt} & = \omega M_y  \\
        \frac{dM_y}{dt} & = -\omega M_x
    \end{aligned}
    \right.
\end{equation}

On cherchera les solutions pour $t\in [t_a=0,t_b=1]$, en imposant les conditions initiales suivantes :
\begin{equation}
    \left\{
    \begin{aligned}
        M_x(0) & = M_0 \\
        M_y(0) & = 0
    \end{aligned}
    \right.
    \label{eq:equations_couplees}
\end{equation}

On pourra ensuite comparer nos résultats avec la solution analytique connue de ce système qui vaut :

\begin{equation}
    \left\{
    \begin{aligned}
        M_x(t) & =  M_0 \cos(2\omega t) \\
        M_y(t) & = -M_0 \sin(2\omega t)
    \end{aligned}
    \right.
    \label{eq:solution_analytique_couplee}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solutions en séries de Fourier}
\label{sec:sol_series_fourier_2D}

On cherche des solutions numériques approchées sous la forme de séries de Fourier tronquées avec $H$ harmoniques, en posant la forme suivante :

\begin{equation}
    \left\{
    \begin{aligned}
        \Tilde{M}_x(t) & = M_0 + \sum_{m=1}^{H} A_m (\cos(m\omega t)-1) + B_m \sin(m\omega t) \\
        \Tilde{M}_y(t) & = \sum_{m=1}^{H} -A_m \sin(m\omega t) + B_m (\cos(m\omega t)-1)
    \end{aligned}
    \right.
    \label{eq:solution_fourier_couplee_2D}
\end{equation}

Les coefficients $(A_m)_{m\in \llbracket 1,H \rrbracket}$ et $(B_m)_{m\in \llbracket 1,H \rrbracket}$ des vecteurs ${\bm A}$ et ${\bm B}$ sont les paramètres à ajuster.
On cherche à obtenir la solution analytique, i.e $\forall m \in\llbracket 1,H \rrbracket, A_m = \delta _1 ^m $ et $\forall m \in\llbracket 1,H \rrbracket, B_m = 0 $.

La fonction de coût pour ces solutions potentielles, en s'intéressant aux $N$ points suivants : $\forall i \in\llbracket 1,N \rrbracket, t_i = \frac{i-1}{N} $ s'écrit

\begin{equation}
    E(\bm{P}) = \sum_{i=1}^{N} (\frac{d\Tilde{M}_x}{dt}(t_i) - \omega \Tilde{M}_y(t_i))^2 + (\frac{d\Tilde{M}_y}{dt}(t_i) + \omega \Tilde{M}_x(t_i))^2,
    \label{eq:erreur_descente_gradient_couplees}
\end{equation}
où ${\bm P}={\bm A}\cup\bm B$.

En utilisant la méthode de descente de gradients, nous calculons les dérivées partielles suivantes :
$(\frac{\partial E}{\partial A_l}, \frac{\partial E}{\partial B_l})_{l \in \llbracket 1,H \rrbracket}$

On peut se ramener à l'écriture du cas à une dimension définissant le vecteur $\bm{P}$ comme l'union des vecteurs $\bm{A}$ et $\bm{B}$ et le vecteur $\bm{g}$ comme le vecteur contenant les dérivées partielles de $E$ par rapport à chaque composante de $\bm{P}$.

\begin{equation}
    \bm{P}^{(k)} = \begin{pmatrix}
        A_1^{(k)} \\
        \vdots    \\
        A_H^{(k)} \\
        B_1^{(k)} \\
        \vdots    \\
        B_H^{(k)}
    \end{pmatrix},
    \bm{g}^{(k)} = \begin{pmatrix}
        \frac{\partial E^{(k)}}{\partial A_1^{(k)}} \\
        \vdots                                      \\
        \frac{\partial E^{(k)}}{\partial A_H^{(k)}} \\
        \frac{\partial E^{(k)}}{\partial B_1^{(k)}} \\
        \vdots                                      \\
        \frac{\partial E^{(k)}}{\partial B_H^{(k)}}
    \end{pmatrix},
    \label{eq:definition_descente_gradients_2D}
\end{equation}

Il existe alors une nouvelle matrice $\mathcal{M}$ d'ordre $2H$ ainsi qu'un nouveau vecteur $\bm{b}$ de taille $2H$ tels que les équations définissant la descente de gradients soient les suivantes :

\begin{equation}
    \bm{P}^{(k+1)} = \bm{P}^{(k)} - \alpha\bm{g}^{(k)}
    \label{eq:recurrence_descente_gradients_2D}
\end{equation}

\begin{equation}
    \bm{g}^{(k)} = \mathcal{M}\bm{P}^{(k)} - \bm{b}
    \label{eq:expression_matricielle_gradient_2D}
\end{equation}

\begin{equation}
    \bm{P}^{(k+1)} = (\mathcal{I}_H - \alpha \mathcal{M} )  \bm{P}^{(k)} + \alpha\bm{b}
    \label{eq:recurrence_descente_gradients_v2_2D}
\end{equation}

On réalise la même étude qu'en dimension 1 pour la recherche du coefficient $\alpha_{min}$ qui minimise le maximum des modules des valeurs propres de $\mathcal{R}_\alpha = \mathcal{I}_H - \alpha \mathcal{M}$, ainsi que le coefficient $\alpha_c$ à ne pas dépasser, à partir duquel ce maximum et supérieur à 1 et la suite diverge.
On trouve les valeurs suivantes $\alpha_{min} = 6.0906.10^{-6}$ et $\alpha_c = 6.1146.10^{-6}$,  et l'évolution du maximum en fonction de $\alpha$ est montré en figure \ref{fig:choix_alpha_2D}.


\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{choix_alpha_2D.jpg}
    \caption{Evolution du maximum du module en fonction de $\alpha$}
    \label{fig:choix_alpha_2D}
\end{figure}

On éxécute l'algorithme en choisissant la valeur $\alpha_{min}$ précédente et on montre l'évolution de l'erreur en fonction du nombre d'itérations en figure \ref{fig:Erreur_2_DG}.
On constate comme dans le cas à 1 dimension que $\alpha_{min}$ donne lieu à une décroissance exponentielle de l'erreur, cette fois pendant les 4500 premières itérations, qui devient ensuite quasiment stationnaire, de l'ordre de $10^{-25}$.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Erreur_2_DG.jpg}
    \caption{Valeur de la fonction d'erreur en fonction du nombre d'itérations}
    \label{fig:Erreur_2_DG}
\end{figure}

Les coefficients finaux de $\bm{P}$ ainsi que les valeurs absolues de leurs erreurs sont représentées en figure \ref{fig:resultats2DG}.
On peut constater que l'on retrouve bien les coefficients attendus, avec une erreur au maximum de l'ordre de $10^{-15}$.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{coefs_2_DG.jpg}
        \caption{Coefficients trouvés}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{coefs_2_DG_erreur.jpg}
        \caption{Valeurs absolue de l'erreur pour chaque coefficient}
    \end{subfigure}
    \caption{Résultats de la méthode de descente de gradients pour le mouvement de précession}
    \label{fig:resultats2DG}
\end{figure}

\section{Solutions aprochées par un réseau de neurones}
\label{section:2_NN}

On cherche dans ce paragraphe à approcher la solution du système des 2 équations grâce à un réseau de neurones.
On choisit l'architecture suivante pour le réseau de neurones :
1 entrée pour la variable temporelle, une première couche intermédiaire de $h_1=32$ neurones, une seconde couche intermédiaire de $h_2=8$ neurones, toutes deux utilisant la sigmoïde définie précédement comme fonction d'activation, et enfin 2 neurones de sortie connectées à la deuxième couche intermédiaire, avec l'identité comme fonction d'activation en sortie.
On note les sorties de ces deux derniers neurones respectivement $\mathcal{N}_x(t, \bm{P})$ et $\mathcal{N}_y(t, \bm{P})$, où $\bm{P}$ représente l'ensemble des paramètres du réseau de neurones.
On construit alors nos solutions approchées de la façon suivante :

\begin{equation}
    \left\{
    \begin{aligned}
        \Tilde{M}_x(t) & = M_0+ t\mathcal{N}_x(t, \bm{P}) \\
        \Tilde{M}_y(t) & = t\mathcal{N}_y(t, \bm{P})
    \end{aligned}
    \right.
    \label{eq:solution_2_NN}
\end{equation}


On définit une fonction d'erreur pour ces solutions potentielles, en s'intérressant aux $N$ points suivants de l'intervalle $[t_a=-1, t_b=1]$:
$\forall i \in\llbracket 1,N \rrbracket, t_i = -1 + 2\frac{i-1}{N} $ :

\begin{equation}
    E(\bm{P}) = \frac{1}{N}\sum_{i=1}^{N} (\frac{d\Tilde{M}_x}{dt}(t_i) - \omega \Tilde{M}_y(t_i))^2 + (\frac{d\Tilde{M}_y}{dt}(t_i) + \omega \Tilde{M}_x(t_i))^2
    \label{eq:erreur_2_NN}
\end{equation}

On réalisera à présent les implémentations en python à l'aide de la bibliothèque TensorFlow.

On choisit ensuite les paramètres d'entraînement suivants : taux d'apprentissage : 0.007, 50000 itérations, et on lance 3 entraînements pour $N \in \{10,40,100\}$.
On obtient les résultats montrés en figure \ref{fig:resultats2NNenfonctiondeN}, où l'axe vertical représente le temps, et les axes horizontaux représentent $M_x$ et $M_y$.
La solution analytique est représentée en orange, la solution approchée est représentée en bleu, et les points rouges sont les points correspondant aux points d'entraînement $(t_i)_{i \in \llbracket 1, N \rrbracket}$.

On constate que pour $N=10$ la solution obtenue est trop éloignée de la solution attendue. Cependant, pour cette durée d'entraînement, il ne semble pas apparaître de différence significative entre $N=40$ et $N=100$.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth, height=0.9\textwidth]{direct_training_N=10.png}
        \caption{Solution obtenue pour $N=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth, height=0.9\textwidth]{direct_training_N=40.png}
        \caption{Solution obtenue pour $N=40$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth, height=0.9\textwidth]{direct_training_N=100.png}
        \caption{Solution obtenue pour $N=100$}
    \end{subfigure}
    \caption{Représentations graphiques de $M_x$ et  $M_y$ en fonction du temps par le réseau de neurones}
    \label{fig:resultats2NNenfonctiondeN}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Précession avec amortissement}
\label{chap:gilbert}

Dans ce chapitre, on s'intéresse à déterminer le mouvement de précession d'un moment magnétique $\bm{M} \in \mathbb{R}^3$ amorti.
Son équation d'évolution est représentée en fonction d'une constante d'amortissement réelle et sans dimension notée $\lambda$ et s'établit, pour simplifier, autour d'un vecteur ${\bm{\omega}}$ pointant le long de l'axe $z$ et dont la composante sur cet axe est constante, soit $\bm{\omega}=\omega_z{\bm z}$.

L'équation considérée prend le nom d'équation de précession avec un terme d'amortissement introduit par Gilbert~\cite{EquationGilbert} et s'écrit

\begin{equation}
    \frac{d\bm{M}}{dt} = (\bm{\omega}-\lambda \frac{d{\bm{M}}}{dt})\times\bm{M}
    \label{eq:equation_Gilbert}
\end{equation}

Cette équation vectorielle admet plusieures propriétés remarquables que nous allons exploiter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Projection en coordonnées cartésiennes}
\label{sec:projection_cartesiennes}

En prenant le produit vectoriel par $\bm M$ à gauche des deux membres de l'équation~\ref{eq:equation_Gilbert}, on forme une projection sur un plan normal à $\bm{M}$ :

\begin{equation}
    \bm{M}\times\frac{d\bm{M}}{dt} = \bm{M}\times((\bm{\omega}-\lambda \frac{d{\bm{M}}}{dt})\times\bm{M} )
    \label{eq:equation_Gilbert_produit_vectoriel}
\end{equation}

Aprés développement, et le changement d'échelle temporelle $\Tilde{t} = (1+\lambda^2)t$, on obtient 3 nouvelles équations différentielles scalaires :

\begin{equation}
    \left\{
    \begin{aligned}
        \frac{dM_x}{d\tilde{t}} & = - M_y\omega_z - \lambda M_x M_z \omega_z \\
        \frac{dM_y}{d\tilde{t}} & = M_x\omega_z - \lambda M_y M_z \omega_z   \\
        \frac{dM_z}{d\tilde{t}} & = \lambda\omega_z (1-M_z^2)
    \end{aligned}
    \right.
    \label{eq:equations_scalaires_Gilbert}
\end{equation}

On remarque que l'équation sur $M_z$ est découplée des 2 autres, on cherche ainsi à résoudre celle-ci en premier lieu en section~\ref{section:Mz}. Puis nous résoudrons les deux équations couplées sur $M_x$ et $M_y$ en section~\ref{section:Mx_My_from_Mz}, en y injectant la solution approchée trouvée pour $M_z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Résolution de l'équation sur $M_z$}
\label{section:Mz}

On utilise la même approche avec un réseau de neurones que précédemment à l'aide d'un réseau de neurones.
On choisit comme architecture du réseau de neurones une entrée pour la variable temporelle, suivie de couches intermédiaires contenant respectivement $h_1$ et $h_2$ neurones, connectés à la sortie unique du réseau de neurones, dont les paramètres sont représenté par le vecteur $\bm{P}$.
Comme précédemment, afin de satisfaire la condition initiale, on cherche les fonction $\tilde{M_z}$ sous la forme suivante, où ${\mathcal{N}}(\Tilde{t},\bm{P})$ représente la sortie du réseau de neurones.

\begin{equation}
    \Tilde{M_z}(\Tilde{t}) = M_z(0) +\Tilde{t} {\mathcal{N}}(\Tilde{t},\bm{P})
    \label{eq:solution_NN_Mz}
\end{equation}

On définit la fonction d'erreur suivante :

\begin{equation}
    E(\bm{P}) = \frac{1}{N} \sum_{i=1}^{N} (\frac{d\Tilde{M}_z}{dt}(t_i) - \lambda\omega_z (1-M_z^2) )^2
\label{eq:erreur_Mz}
\end{equation}

On fixe les paramètres suivants : $\lambda = -0.3$, nombre de points de tests : $N=30$, $h_1 = 16$, $h_2 = 8$, intervalle d'étude : $[t_a = -1, t_b = 1]$, $M_z(0) = 0$.
Lors des essais d'entraînement du modèle on fait face à plusieurs problèmes.
On remarque que souvent la sortie du réseau de neurones est quasiment constante sur l'intervalle d'étude.
Ainsi, il semble que l'entraînement ne consiste alors dans ce cas qu'à ajuster cette valeur constante pour minimser la fonction d'erreur dans le cas où $M_z$ est linéaire par rapport à $\tilde{t}$, (d'après l'équation \ref{eq:solution_NN_Mz},pour ${\mathcal{N}}(\Tilde{t},\bm{P}) = \mu\in \mathbf{R}$ constante, et $M_z(0) = 0$).

Après plusieurs essais d'entraînement, on remarque que l'on tombe sur l'une ou l'autre des 2 solutions présentées en figure \ref{fig:minima_locaux_Mz}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{min_loc1.png}
        \caption{Première solution linéaire}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{min_loc2.png}
        \caption{Seconde solution linéaire}
    \end{subfigure}
    \caption{Solutions trouvées pour $M_z$ après de rapides entraînements}
    \label{fig:minima_locaux_Mz}
\end{figure}

On peut vérifier cette hypothèse en exprimant analytiquement notre fonction d'erreur pour une fonction $M_z$ linéaire : $\forall \tilde{t} \in \mathbf{R}, \tilde{M_z}(\tilde{t}) = \mu  \tilde{t}$.
On peut calculer notre erreur, en fonction de $\mu$, en remplaçant la somme discrète par une intégrale qui peut être calculée facilement car
l'intégrande est alors une fonction polynomiale en $\tilde{t}$ :

\begin{equation}
    \begin{aligned}
        E(\mu) & = \frac{1}{t_b-t_a}\int_{t_b}^{t_a} (\frac{d\tilde{M_z}}{d\tilde{t}}(\tilde{t})-\lambda \omega_z (\tilde{M_z}(t)^2-1))^2 \,d \tilde{t} \\
        E(\mu) & = \frac{16}{5}\lambda^2\omega_z^2\mu^4
        - \frac{8}{3}\lambda\omega_z\mu^3
        + (1- \frac{8}{3}\lambda^2\omega_z^2)\mu^2
        + 2\lambda\omega_z\mu
        + \lambda^2\omega_z^2
    \end{aligned}
    \label{eq:fonction_erreur_analytique_M_z_lineaire}
\end{equation}

On remarque alors que cette fonction admet deux minima locaux qui ont pour valeurs approchées : $ \mu_a \approx 0.696$, $E(\mu_a) \approx 3.04$ et $\mu_b \approx -0.574$, $E(\mu_b) \approx -0.78$.
Cela est effectivement proche des valeurs que l'on obtient lors des entraînements montrés en figure \ref{fig:minima_locaux_Mz}.
En effet, sur le premier exemple, la sortie du réseau de neurones varie entre 0.60 et 0.74 sur l'intervalle étudié, et la valeur finale de l'erreur est 3.04, ce qui correspond à $\mu_b$ et $E(\mu_b)$.
Pour le second exemple, la sortie est comprise entre -0.60 et -0.52, et l'erreur vaut 0.82, ce qui correspond bien au second minimum local $E(\mu_a)$.

On peut alors remettre en question le modèle choisi pour notre réseau de neurones, qui n'est pas forcément le plus adapté.
On peut par exemple essayer de changer les fonctions d'activation utilisées par les couches intermédiaires, qui étaient des sigmoïdes, et les remplacer par la fonction $relu$.

On a ainsi pu obtenir des résultats satisfaisants visibles en figure \ref{fig:relu_apprentissage_direct} en lançant l'entraînement avec les fonctions $relu$ pour 10000 itérations, avec un taux d'apprentissage de $10^{-3}$, $N=50$, $[t_a = -2, t_b = 2]$.
L'erreur finale est de 0.018.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{direct_relu_0_001_10000_32_16_v2.png}
    \caption{Résultats obtenus avec la fonction d'activation $relu$}
    \label{fig:relu_apprentissage_direct}
\end{figure}

Une autre tentative consiste à simplifier l'architecture du réseau de neurones en supprimant la seconde couche intermédiaire pour ne conserver que la première et ainsi diminuer fortement la dimension de l'espace dans lequel on cherche le meilleur vecteur $\bm P$. Avec les paramètres d'entraînement suivants : 100000 itérations, avec un taux d'apprentissage de $1.10^{-2}$, $N=50$, $[t_a = -2, t_b = 2]$.
L'erreur finale est de $2.7.10^{-4}$, mais continue à décroître.
On voit en figure~\ref{fig:Mz_sigmoid_1_couche} que les courbes des solutions exactes et approchées se confondent.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Mz_50000_epochs_1_couche.PNG}
    \caption{Résultats obtenus avec la fonction d'activation $relu$}
    \label{fig:Mz_sigmoid_1_couche}
\end{figure}


\subsection{Résolution des équations sur Mx et My}
\label{section:Mx_My_from_Mz}

On reprend dans ce paragraphe l'approche de la section~\ref{section:2_NN}, pour résoudre le système décrivant l'évolution des composantes sur $x$ et sur $y$, en utilisant pour $M_z$ la solution approchée établie dans la section précédente.

On choisit l'architecture suivante pour le réseau de neurones : 
1 entrée pour la variable temporelle, une unique couche intermédiaire de $h_1=16$ neurones, utilisant la sigmoïde comme fonction d'activation, et enfin 2 neurones de sortie, avec l'identité comme fonction d'activation en sortie.
Ces deux sorties sont notées respectivement $\mathcal{N}_x(t, \bm{P})$ et $\mathcal{N}_y(t, \bm{P})$, ce qui nous permet de définir comme précédemment nos fonctions candidates :

\begin{equation}
    \left\{
        \begin{aligned}
            \Tilde{M}_x(t) &= M_{x0} + t\mathcal{N}_x(t, \bm{P}) \\
            \Tilde{M}_y(t) &= M_{y0} + t\mathcal{N}_y(t, \bm{P})
        \end{aligned}
    \right.
    \label{eq:solution_Mx_My_from M_z}
\end{equation}

On définit une fonction d'erreur pour ces solutions potentielles, en s'intéressant aux $N$ points suivants de l'intervalle $[t_a=-1, t_b=1]$: 
$\forall i \in\llbracket 1,N \rrbracket, t_i = -1 + 2\frac{i-1}{N} $ :

\begin{equation}
    \begin{aligned}
        E(\bm{P})=&\frac{1}{N}\sum_{i=1}^{N}\left((\frac{d\Tilde{M}_x}{dt}(t_i) + M_y\omega_z + \lambda M_x M_z \omega_z)^2\right.\\
        &\left.+(\frac{d\Tilde{M}_y}{dt}(t_i) -M_x\omega_z + \lambda M_y M_z \omega_z)^2\right)
    \end{aligned}
    \label{eq:erreur_Mx_My_from_Mz}
\end{equation}
On choisit ensuite les paramètres d'entraînement suivants : taux d'apprentissage de 0.01, et 60000 itérations.
Les résultats sont regroupés dans la figure \ref{fig:resultatsMxMy_from_Mz}.
La solution analytique est représentée en orange, la solution approchée est représentée en bleu, et les points rouges sont les points correspondant aux points d'entraînement $(t_i)_{i \in \llbracket 1, N \rrbracket}$.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth, height=0.9\textwidth]{Mx_from_Mz.PNG}
        \caption{Solution obtenue pour $M_x$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth, height=0.9\textwidth]{my_from_mz.PNG}
        \caption{Solution obtenue pour $M_y$}
    \end{subfigure}
       \caption{Représentations graphiques de $M_x$ et  $M_y$ en fonction du temps par le réseau de neurones}
       \label{fig:resultatsMxMy_from_Mz}
\end{figure}
 
On peut montrer que l'équation~\ref{eq:equation_Gilbert} implique la conservation de la norme de $M$. 
En effet, en prenant le produit scalaire des deux membres de l'équation par $M$, il vient :
\begin{equation}
    \bm{M} \cdot \frac{d\bm{M}}{dt} = 0 \Leftrightarrow 
    \frac{d\lVert \bm{M} \rVert ^2}{dt} = 0 
\label{eq:conservation_norme_M}
\end{equation}

On peut vérifier que cette condition est respectée par le modèle, sachant que la condition initiale impose $\lVert \bm{M} \rVert=1$.
On trace pour cela $\lVert \bm{M} \rVert$ en fonction du temps en figure~\ref{fig:norme_de_M}, et on constate qu'elle reste comprise entre 0.975 et 1 sur tout l'intervalle d'étude.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{norm over time.PNG}
    \caption{Norme de l'aimantation en fonction du temps}
    \label{fig:norme_de_M}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion et perspectives}
\label{chap:conclusion}

L'étude menée dans le chapitre~\ref{chap:ode_1} a permis de comprendre le principe de la résolution d'une équation différentielle par un réseau de neurones, et de montrer que cette approche permettait de résoudre une équation simple à une dimension.
Le chapitre~\ref{chap:precession} a ensuite montré que cette approche donnait également des résultats concluants dans le cas d'un système de deux équations couplées décrivant un mouvement de précession.
Cela a également donné l'occasion de prendre en main la librairie TensorFlow afin d'utiliser les méthodes de différentiation automatique pour le calcul de la dérivée de la fonction candidate ainsi que pour celui des gradients de le fonction de coût.
Enfin le chapitre~\ref{chap:gilbert} a montré que l'approche mise en place permettait de retrouver de façon approchée les solutions analytiques de l'équation de précession avec amortissement, dans le cas où celle-ci est projetté dans le plan normal à $\bm M$.


Il serait utile d'améliorer les permorfances des algorithmes utilisés, en effet les entraînements des réseaux de neurones présentés ici ont rarement atteint une fonction de coût stationnaire. 
Les entraînements duraient jusqu'à plusieurs dizaines de minutes, et je n'ai pas choisi de les poursuivre plus longtemps lorsque le résultat obtenu était suffisament proche des solutions analytiques. 
L'amélioration des performances d'entraînement pourrait être réalisée en essayant par exemple des algorithmes d'optimisation plus avancés que la descente de gradients, et également en utilisant d'autres méthodes d'entraînement plus performantes fournies par TensorFlow.
On pourrait alors approfondir l'étude des différences de précision selon les différentes architectures de réseaux de neurones pour identifier lesquelles sont les plus pertinentes.

Il serait aussi intéressant d'utiliser l'approche présentée dans ce rapport pour tenter de résoudre directement l'équation de Landau--Lifshitz-\-Gilbert~\cite{EquationGilbert} sans projection, afin de constater si cela permet de retrouver les mêmes solutions que l'équation projetée. 

On peut ensuite envisager d'étudier de façon similaire l'équation obtenue en remplaçant le terme d'amortissement par un terme de nutation~\cite{Neeraj2021}, ainsi que l'équation prenant à la fois en compte l'amortissement et la nutation.


\begin{thebibliography}{8}
    \bibitem{PrecessionLarmor}{\href{https://fr.wikipedia.org/wiki/Pr%C3%A9cession_de_Larmor}{''Précession de Larmor'' (2022) {\it Wikipédia}}}
    \bibitem{EquationGilbert}{\href{https://fr.wikipedia.org/wiki/%C3%89quation_de_Landau-Lifshitz-Gilbert}{"Équation de Landau-Lifshitz-Gilbert" (2022), {\it Wikipédia}}}
    \bibitem{MLWithApp}{\href{https://doi.org/10.1016/j.mlwa.2021.100058}{T.T.Dufera, Machine Learning With Applications {\bf 5}, 100058 (2021)}}
    \bibitem{ANNforOPDEs}{\href{https://doi.org/10.1109/72.712178}{I. E. Lagaris, A. Likas and D. I. Fotiadis, IEEE Transactions on Neural Networks {\bf 9} no.5, 987-1000 (1998)}}
    \bibitem{HighOrderHybrid}{\href{https://doi.org/10.1016/j.amc.2006.05.068}{A.Malek and R.Shekari Beidokhti, Applied Mathematics and Computation {\bf 183}, 260–271 (2006)}}
    \bibitem{FunctionApproximation}{\href{https://dl.acm.org/doi/10.5555/1466915.1466916#}{Z.Zainuddin and P.Ong, WSEAS Transactions on Mathematics {\bf 7} no.6, 333–338 (2008)}}
    \bibitem{WikiNN}{\href{https://en.wikipedia.org/wiki/Feedforward_neural_network}{"Réseaux de neurones" (2022), {\it Wikipédia}}}
    \bibitem{WikiRayonSpectral}{\href{https://en.wikipedia.org/wiki/Spectral_radius#Power_sequence}{"Rayon spectral et suites des puissances" (2022), {\it Wikipédia}}}
    \bibitem{Neeraj2021}{\href{https://www.nature.com/articles/s41567-020-01040-y}{K. Neeraj {\it et al}. Nature Physics {\bf 17}, 245-250 (2021)}}
\end{thebibliography}

\end{document}