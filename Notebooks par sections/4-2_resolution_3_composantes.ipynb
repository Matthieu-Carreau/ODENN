{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook correspondant à la partie 4.2 du rapport, pour la résolution de l'équation avec amortissement sur les 3 composantes simultanément\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "    \\begin{aligned}\n",
    "        \\frac{dM_x}{dt} &=-\\omega_z M_y - \\lambda(M_z \\frac{dM_y}{dt} - M_y \\frac{dM_z}{dt}) \\\\\n",
    "        \\frac{dM_y}{dt} &= \\omega_z M_x - \\lambda(M_x \\frac{dM_z}{dt} - M_z \\frac{dM_x}{dt}) \\\\\n",
    "        \\frac{dM_z}{dt} &= - \\lambda(M_y \\frac{dM_x}{dt} - M_x \\frac{dM_y}{dt})\n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_0 = tf.constant([1,0,0], dtype='float32')\n",
    "W = 2*np.pi\n",
    "lamb = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32 #number of samples for the independant variable\n",
    "t_a = -1\n",
    "t_b = 1\n",
    "training_points = np.linspace(t_a, t_b,N)\n",
    "training_points = tf.convert_to_tensor(training_points, dtype=tf.float32)\n",
    "\n",
    "load_model = False\n",
    "load_filename = \"models/resolution_sans_projection\"\n",
    "save_model = False\n",
    "save_filename = \"models/resolution_sans_projection\"\n",
    "learning_rate = 1e-4\n",
    "epochs = 0\n",
    "display_step = min(max(1,epochs//100), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 1     # input layer number of neurons\n",
    "n_hidden_1 = 32 # 1st layer number of neurons\n",
    "n_output = 3    # output layer number of neurons\n",
    "\n",
    "#model definition :\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid, input_shape=(n_input,)),  # input shape required\n",
    "  tf.keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "if load_model :\n",
    "    model = tf.keras.models.load_model(load_filename)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, input_tensor, M_0):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(input_tensor)\n",
    "        output = model(input_tensor)\n",
    "        Mx = M_0[0]+input_tensor*output[:,0]\n",
    "        My = M_0[1]+input_tensor*output[:,1]\n",
    "        Mz = M_0[2]+input_tensor*output[:,2]\n",
    "\n",
    "    dMx = tape.gradient(Mx, input_tensor)\n",
    "    dMy = tape.gradient(My, input_tensor)\n",
    "    dMz = tape.gradient(Mz, input_tensor)\n",
    "\n",
    "    ex = dMx + W*My + lamb*(Mz*dMy - My*dMz)\n",
    "    ey = dMy - W*Mx + lamb*(Mx*dMz - Mz*dMx)\n",
    "    ez = dMz + lamb*(My*dMx - Mx*dMy)\n",
    "\n",
    "    return tf.reduce_mean(ex**2 + ey**2 + ez**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, input_tensor, M_0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_function(model, input_tensor, M_0)\n",
    "\n",
    "    gradient = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    return loss_value, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=7e-3)\n",
    "losses = []\n",
    "epochs_displayed = []\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    loss_value, grads = grad(model, training_points, M_0)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if epoch % display_step == 0 :\n",
    "        print(\"Loss after\",epoch,\"/\",epochs,\"epochs :\",loss_value.numpy())\n",
    "        losses.append(loss_value.numpy())\n",
    "        epochs_displayed.append(epoch)\n",
    "\n",
    "loss_value, grads = grad(model, training_points, M_0)\n",
    "print(\"Final loss after\",epochs,\"epochs :\",loss_value.numpy())\n",
    "losses.append(loss_value.numpy())\n",
    "epochs_displayed.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model :\n",
    "    model.save(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_displayed, losses)\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_plotting_points = 200\n",
    "plotting_points = np.linspace(t_a,t_b,nb_plotting_points)\n",
    "plotting_points = tf.convert_to_tensor(plotting_points, dtype=tf.float32)\n",
    "\n",
    "time_factor = 1 + lamb**2\n",
    "\n",
    "#for plotting_points\n",
    "K = (M_0[2]-1) / (M_0[2]+1)\n",
    "exp_vector = tf.exp(-lamb*W*plotting_points/time_factor)\n",
    "F = exp_vector*(K-1)/(K*exp_vector**2-1)\n",
    "\n",
    "Mx_ana =tf.multiply( F,(  tf.cos(W*plotting_points/time_factor)*M_0[0] - tf.sin(W*plotting_points/time_factor)*M_0[1]))\n",
    "My_ana = tf.multiply(F,( tf.sin(W*plotting_points/time_factor)*M_0[0] + tf.cos(W*plotting_points/time_factor)*M_0[1]))\n",
    "Mz_ana = (1+K*exp_vector**2) / (1-K*exp_vector**2)\n",
    "\n",
    "#for training_points\n",
    "K = (M_0[2]-1) / (M_0[2]+1)\n",
    "exp_vector = tf.exp(-lamb*W*training_points/time_factor)\n",
    "F = exp_vector*(K-1)/(K*exp_vector**2-1)\n",
    "\n",
    "Mx_training =tf.multiply( F,(  tf.cos(W*training_points/time_factor)*M_0[0] - tf.sin(W*training_points/time_factor)*M_0[1]))\n",
    "My_training = tf.multiply(F,( tf.sin(W*training_points/time_factor)*M_0[0] + tf.cos(W*training_points/time_factor)*M_0[1]))\n",
    "Mz_training = (1+K*exp_vector**2) / (1-K*exp_vector**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimation and the analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network estimation\n",
    "output = model(plotting_points)\n",
    "Mx_NN = M_0[0]+plotting_points*output[:,0]\n",
    "My_NN = M_0[1]+plotting_points*output[:,1]\n",
    "Mz_NN = M_0[2]+plotting_points*output[:,2]\n",
    "\n",
    "plt.plot(plotting_points, Mx_NN, label='Mx obtenue')\n",
    "plt.plot(plotting_points, Mx_ana, label = 'Mx théorique')\n",
    "plt.scatter(training_points, Mx_training, label = \"points d'entraînement\", color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plotting_points, My_NN, label='My obtenue')\n",
    "plt.plot(plotting_points, My_ana, label = 'My théorique')\n",
    "plt.scatter(training_points, My_training, label = \"points d'entraînement\", color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plotting_points, Mz_NN, label='Mz obtenue')\n",
    "plt.plot(plotting_points, Mz_ana, label = 'Mz théorique')\n",
    "plt.scatter(training_points, Mz_training, label = \"points d'entraînement\", color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "norm = tf.sqrt(Mx_NN**2 + My_NN**2 + Mz_NN**2)\n",
    "plt.plot(plotting_points, norm, label='norme obtenue')\n",
    "plt.plot(plotting_points, np.ones((nb_plotting_points)), label = 'norme théorique')\n",
    "plt.scatter(training_points, np.ones((N)), label = \"points d'entraînement\", color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
