{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook correspondant à la section 4.1.1 du rapport, pour la résolution de l'équation sur  $M_z$ pour le mouvement de précession avec amortissement.\n",
    "\\begin{equation}\n",
    "    \\frac{dM_z}{d\\tilde{t}} = \\lambda\\omega_z (1-M_z^2) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_z0 = tf.constant(0, dtype='float32')\n",
    "W = 2*np.pi\n",
    "lamb = -0.3\n",
    "\n",
    "t_a = -2\n",
    "t_b = 2\n",
    "\n",
    "t_0 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the ODE : this function returns the value expected of the derivative, given the independant variable and the value of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODE(T, Mz, lamb, W) :\n",
    "    return lamb*W*(1-Mz**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 #number of samples for the independant variable\n",
    "training_points = np.linspace(t_a,t_b,N)\n",
    "training_points = tf.convert_to_tensor(training_points, dtype=tf.float32)\n",
    "\n",
    "load_model = False\n",
    "load_filename = \"models/4_1_1_Mz\"\n",
    "save_model = False\n",
    "save_filename = \"models/4_1_1_Mz\"\n",
    "learning_rate = 1e-2\n",
    "epochs = 10000\n",
    "display_step = min(max(1,epochs//100), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 1     # input layer number of neurons\n",
    "n_hidden_1 = 8 # 1st layer number of neurons\n",
    "n_output = 1    # output layer number of neurons\n",
    "\n",
    "#tf.random.set_seed(24514)\n",
    "\n",
    "#model definition :\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid, input_shape=(n_input,)),\n",
    "  tf.keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "if load_model :\n",
    "    model = tf.keras.models.load_model(load_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, input_tensor, M_z0, lamb, W):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(input_tensor)\n",
    "        output = model(input_tensor, training=False)\n",
    "        Mz = M_z0+input_tensor*output[:,0]\n",
    "\n",
    "    dMz = tape.gradient(Mz, input_tensor)\n",
    "\n",
    "    e = dMz - ODE(input_tensor, Mz, lamb, W)\n",
    "\n",
    "    return tf.reduce_mean(e**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, input_tensor, M_z0, lamb, W):\n",
    "    with tf.GradientTape() as tape2:\n",
    "        loss_value = loss_function(model, input_tensor, M_z0, lamb, W)\n",
    "    gradient = tape2.gradient(loss_value, model.trainable_variables)\n",
    " \n",
    "    return loss_value, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "losses = []\n",
    "epochs_displayed = []\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    loss_value, grads = grad(model, training_points, M_z0, lamb, W)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if epoch % display_step == 0 :\n",
    "        print(\"Loss after\",epoch,\"/\",epochs,\"epochs :\",loss_value.numpy())\n",
    "        losses.append(loss_value.numpy())\n",
    "        epochs_displayed.append(epoch)\n",
    "\n",
    "loss_value, grads = grad(model, training_points, M_z0, lamb, W)\n",
    "print(\"Final loss after\",epochs,\"epochs :\",loss_value.numpy())\n",
    "losses.append(loss_value.numpy())\n",
    "epochs_displayed.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if save_model :\n",
    "    model.save(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_displayed, losses)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic(T, M_z0, lamb, W) :\n",
    "    K = (M_z0-1) / (M_z0+1)\n",
    "    Kexp = K*np.exp(-2*lamb*W*T)\n",
    "    Mz_ana = (1+Kexp) / (1-Kexp)\n",
    "    return Mz_ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimation and the analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the estimation\n",
    "nb_plotting_points = 200\n",
    "plotting_points = np.linspace(t_a,t_b,nb_plotting_points)\n",
    "plotting_points = tf.convert_to_tensor(plotting_points, dtype=tf.float32)\n",
    "\n",
    "#neural network estimation\n",
    "output = model(plotting_points).numpy().reshape((nb_plotting_points))\n",
    "Mz_NN = M_z0 + (plotting_points-M_z0)*output\n",
    "\n",
    "\n",
    "print(\"Extreme values of output after :\",min(output), max(output))\n",
    "#analytic solution\n",
    "Mz_ana = analytic(plotting_points, M_z0, lamb, W)\n",
    "\n",
    "#training points\n",
    "Mz_ana_training = analytic(training_points, M_z0, lamb, W)\n",
    "\n",
    "\n",
    "plt.plot(plotting_points, Mz_NN, label='solution approchée')\n",
    "plt.plot(plotting_points, Mz_ana, label='solution exacte')\n",
    "plt.scatter(training_points, Mz_ana_training, label=\"points d'entraînement\", color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('résolution par réseau de neurones')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
